{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eYgyZKrAEWe"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gXr1cm_15-T",
    "outputId": "b9d0d2ca-a9d3-44f2-b73d-85fe8fdcaeaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "#library\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# \ub79c\ub364 \uc2dc\ub4dc \uace0\uc815 (\uc7ac\ud604\uc131 \ud655\ubcf4)\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# GPU \ud655\uc778\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376,
     "referenced_widgets": [
      "34bf21f0e81b4f928752e76ff5150880",
      "33f94b0f9daf472282161a70f5ce979b",
      "20536f2649394a1fa74c145deb3c9413",
      "976af2ea02e841d7bcc068214a91f85c",
      "135b28513ee547308b854ae3f7961322",
      "098c6345bc4a4e33bd63f7730895934e",
      "36612bcadb37453c8bd68d24c0df4633",
      "93e0c1a6795c4062b8c77fdf69c94d7d",
      "5865e0f286df423b815f83728407afe6",
      "40d9f82f33dd4c3f9c16e6eceb0ddb80",
      "d79324de526c480a9d7b673b4dade8d0",
      "a0b2d93ce8e140f9b089be55a68bf822",
      "847fe7199a4f486fb4c8d5e24e2503d6",
      "22813387e4f242b7be4c47c56d580e99",
      "6db7cc788fa6413ba3d061e04c362b93",
      "027dfcfe3e984c5cb564351ab07c6ad9",
      "6990948a31b74aea964d00664579aee0",
      "10671b540bfc4ebdb6402cb36513ff8f",
      "05f9c75c70b348598e7ed009898aa47d",
      "87ee75b06a8a420682b5bce8f2767694",
      "d64045e74d4841c1a48e1b0e96995dc6",
      "8b55bf1eab69428ea450540f4d5d3621",
      "17fba2174f8f453eb46e2a3da3b8f8d6",
      "05c9b0882e4542949da878b905175479",
      "792405657e8a468384041ef2712bc910",
      "d7f48dfd26bb47b4ba780b2a243d7466",
      "11584a91c00441b9b55bce6b33f7947d",
      "59cf7e32f34946cf9336ef4d31d67e4d",
      "c2a77d66b1c64183b89fee21636ea365",
      "8b5acc5fff3e45c4849136bfe44b17af",
      "5714681d7a674b169ab94209d15fa25d",
      "b7a4067d000f410698e1865512cae6d4",
      "c5ff8d3d2b1a413496da1d974c9cac45",
      "1b59527016034fe69c84e29d0da98dfa",
      "40c76a2b352a45f991f85365e6ce4cd3",
      "a8ffcaaf239c43e9b3b190e62ae0f0fa",
      "0795b3c4742f4d89a3878441a6aed138",
      "7a87b9a68964456696f5c5f511e43478",
      "eaaf5e745ca449c89d5c0bb3397779fd",
      "43846f2a895f4a93a0c38e456a11afd9",
      "dce638f73a1b45f3bbf14809a79896e0",
      "1a5ddc304a1e40aab880fd383e3cac91",
      "4fe6cf9c4ecb40eaa28a0744e63af4f0",
      "6fdfbfd977244e9f865143edc654e3cf",
      "1197c57b3a12413ea73f1e0cc00a7854",
      "b16b8e32287b489b895419b3f79be149",
      "51e8f62de81543ea97b28d56e0ec759e",
      "0303dc86a7d949e09ddc5fb04a343ef6",
      "fbef88768ec34c0b9c0ef92d37fc760a",
      "eae7f7e15ef24d4da971d053ecfa4d62",
      "59083b8a03c54e1bb0dc98d431551870",
      "6dca7688fb7c428dbdd41ac5160b36d2",
      "e8fdc8c54b3e4f67ac9f42baed8c91ae",
      "e30ad4f941964759a4581b92e7e0719f",
      "f00ed21e42334fe992374474fc0bb5a0",
      "8fedbde3dfdc4626bc2251eea33e999b",
      "cf694b4dceac4fb7bfa16bae9ef423d2",
      "fdebd1dd783041a1a044d7bcff55c5ff",
      "cebe01e3bf614801872b9203b4c8912e",
      "735615e61b9f4dbb9d3b78f7dd5dd83b",
      "401879ffcaab4567837bc019b322e02d",
      "ee683880397641ccbc51b1f163fa3740",
      "45a9c2ebdcfe4a1088b1e0606e3fddaf",
      "8409c5a411694332ad86aa4405b77d1a",
      "ad8f3f769e8b494fa348ce02e9326083",
      "b9aa6e8f4c4b4451b7a56cb9ab8123a8",
      "730656f1f9994989a6c0c02e865b1bb6",
      "e62d9526ddda442eb4ff65c0b75499c4",
      "6cc8d9478de748788d6a37baa8481a77",
      "894f37b3aef846ccbd540fd63b801c36",
      "a2bebace66df4e6e8fbae48a44f2ab97",
      "e8131568a65a4182a4edce409525ed1b",
      "42ce77ebfd744be3af0fa63de98fa95c",
      "4067b290c084428dabff44635c94b128",
      "0c7282851c9e4c46a41dc7ad993ff092",
      "220922d260714b4aa7ede845702e7c04",
      "199df6f47f8b45a3b500d94abaa2b225",
      "da6e9c4f5365481994aae6215dc431f3",
      "11abb38ecea6415dbcf39dd8c27c662e",
      "e2e094d71ac649dcbaff2a0fc69161ab",
      "063c8589cda54e84bb3344da290d6ebc",
      "1df8ede3ecb44af6b0e005ec3d8375ec",
      "683df5ea3c8a4f99a0d9a9936021a2f7",
      "305621eb1c1740dd887ddb95b0e9a042",
      "448b566ad52647389a5a7761319f456d",
      "1b7afaecbf964418b3ae8a551cf9d747",
      "334b86ab03474af9890621ad012f3b96",
      "7fa014ff5ab84b1584c21cf2c0a42e24",
      "cab55868ddab4864ad2bfc6692ebe744",
      "4b24a98638294186835544db6b1cedd5",
      "1cc9ca8591b34357b8a5df6809e03dec",
      "66994fd303984ceb99190520e1f4b3b1",
      "be79654a3de54bef828c8f3461506608",
      "e266879f0eb84c63996282aa79969ad9",
      "0b923fbdcb3f45368732e4b1f4924fda",
      "ae6bdad6c821462b8714e98438ee1577",
      "674cdee62bc24b478d31194cfbb6a03b",
      "8e6c72d5a48442e89f3b261b7cde73bd",
      "1cd95c9232a14f548edecb4c6041d5e4"
     ]
    },
    "id": "5G1t3_LJ17zV",
    "outputId": "34f9fef5-3743-4932-850c-1b69f31aae52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bf21f0e81b4f928752e76ff5150880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b2d93ce8e140f9b089be55a68bf822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fba2174f8f453eb46e2a3da3b8f8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b59527016034fe69c84e29d0da98dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 150000\n",
      "Test set size: 50000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1197c57b3a12413ea73f1e0cc00a7854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fedbde3dfdc4626bc2251eea33e999b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730656f1f9994989a6c0c02e865b1bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6e9c4f5365481994aae6215dc431f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab55868ddab4864ad2bfc6692ebe744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 150000\n",
      "Test set size: 50000\n"
     ]
    }
   ],
   "source": [
    "#load data from github\n",
    "from datasets import load_dataset\n",
    "\n",
    "#load nsmc daa in csv file format\n",
    "data_files = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "}\n",
    "\n",
    "# use csv instead of the nsmc and use delimiter\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "# check if data loading is done alright\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")\n",
    "\n",
    "#load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# check the data\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "4457bf74721647dd9f71bbfcf4dd7ebc",
      "c0b090edadbc4e93954d99bfbc240faa",
      "1b74f0bf406c4138b7df72019ef7e7b2",
      "b43e672d029b4170a9b944f320a1ac4f",
      "aa652c000c884eca9764df36d7d77cb4",
      "9daeb3a5299b4e8298454a6562a1c5ba",
      "487c3188b1e54034a3fd3d232da3dfd6",
      "f60402f2a120457288223910933b9b5f",
      "107123f7f0a44db8acd32ae723162d96",
      "b23cb3b9010b4649a83eb93d895c3a36",
      "299123f4a17d4035ae7a15697ec6cd6d",
      "1ed0376fef87427484879ffdca72ba55",
      "72ff712e68a84697b2ee3e008c72c58f",
      "1bf057eb35ce4ef689622c0223cad201",
      "8422a546bd6a434eaee311e161a010ab",
      "5492778e6ed04696821844206d5fe85b",
      "b68c5273810642c5a2949ba6a801d216",
      "8febaee7ef47438e9b67a93243e3b950",
      "f78b60d62349425786cc446a66154e3e",
      "1f9c7aa164484fb4afd69fd915b62bdd",
      "deaae1fa0959437f9349b76817eacb72",
      "9460521c63554a79b2972900b662ec2b",
      "f2fd75b6d02044fea57a540dc8400e87",
      "0625198322534d0eb04da2b3d1845864",
      "a5b22cb5fbff4ef38607407e180721c2",
      "40d97d21a69e4258aba7a2ea32939375",
      "12b4124889c74752908bebe48a46b9ef",
      "2a000ea632984a3aadea1607340c7126",
      "95ef812fe4334ca994b06408d45db499",
      "1f2bd8433dee4ae8a527164d3b41110e",
      "d006c2757ec54b52ab679ddfe0d7b938",
      "1c3ab50ffdf6412b896a44751d1b2471",
      "525005cdbf134c8683295b6a21625689",
      "345619991c434642b7a9f9463c02a8c9",
      "22a90105c5364ad383e286c8a5a48687",
      "93b8976886e04aad8b35fd7e8dd7e71f",
      "83a66254ca5941e99d3274f054833765",
      "7eec558d72af498dbe6a629ac8e179e2",
      "a8c5fb718b7c4dfd93830d414859562c",
      "68df7b3e1d174b1c82676c7dba56ed0d",
      "77964b580dab4179911093728ac2ed82",
      "98e4413d6b6945e1bc4d0be61f654225",
      "6476c6672a72401182264aa45426b086",
      "c0f3785a619c403c904bf730845ac1c3"
     ]
    },
    "id": "fXQxqzjJ17wf",
    "outputId": "a62cc41f-8cdb-4c75-eee5-28141d3b75c2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get rid of Nan\n",
    "dataset = dataset.filter(lambda x: x['document'] is not None)\n",
    "\n",
    "# define preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # max_length\ub294 128 \uc815\ub3c4\uba74 NSMC \ubd84\uc11d\uc5d0 \ucda9\ubd84\ud558\uba70 \uc18d\ub3c4\ub3c4 \ube60\ub984\n",
    "    return tokenizer(\n",
    "        examples[\"document\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=False # Dynamic Padding\uc744 \uc704\ud574 \uc5ec\uae30\uc11c\ub294 \ud328\ub529\ud558\uc9c0 \uc54a\uc74c\n",
    "    )\n",
    "\n",
    "# tokenize the entire dataset (use batch_size for parallel processing)\n",
    "encoded_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# get rid of useless columns (Yeahhhhhh get rid of them!!!)\n",
    "encoded_datasets = encoded_datasets.remove_columns([\"id\", \"document\"])\n",
    "encoded_datasets = encoded_datasets.rename_column(\"label\", \"labels\")\n",
    "encoded_datasets.set_format(\"torch\")\n",
    "\n",
    "#data split (one for train set and another for test set)\n",
    "train_dataset = encoded_datasets[\"train\"]\n",
    "eval_dataset = encoded_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g4dFH8dBAqPd"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #choose the logit with the highest value as a predicted value\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "ugbw9BZL17uB",
    "outputId": "3c5fd945-f7c4-4c7f-ef6e-fb87c6b98297"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-913363049.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_step4 = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> STEP 4: Training WITHOUT Bucketing...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14064' max='14064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14064/14064 33:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.238908</td>\n",
       "      <td>0.901814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.258919</td>\n",
       "      <td>0.906874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.305589</td>\n",
       "      <td>0.905454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 00:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4 Training Time: 1988.23 seconds\n",
      "STEP 4 Accuracy: 0.9069\n"
     ]
    }
   ],
   "source": [
    "# reinitialize a model\n",
    "def get_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "# Dynamic Padding Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# training w/o bucketing\n",
    "training_args_step4 = TrainingArguments(\n",
    "    output_dir=\"./results_step4\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    group_by_length=False,\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer_step4 = Trainer(\n",
    "    model=get_model(),\n",
    "    args=training_args_step4,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# measure time spent\n",
    "print(\">>> STEP 4: Training WITHOUT Bucketing...\")\n",
    "start_time_4 = time.time()\n",
    "train_result_4 = trainer_step4.train()\n",
    "end_time_4 = time.time()\n",
    "\n",
    "# save results\n",
    "step4_time = end_time_4 - start_time_4\n",
    "step4_acc = trainer_step4.evaluate()['eval_accuracy']\n",
    "\n",
    "print(f\"STEP 4 Training Time: {step4_time:.2f} seconds\")\n",
    "print(f\"STEP 4 Accuracy: {step4_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "-Jrd-ABw17rq",
    "outputId": "b7a264db-a710-4399-cfbe-b47a439fc98b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-2954867252.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_step5 = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> STEP 5: Training WITH Bucketing...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14064' max='14064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14064/14064 22:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.250168</td>\n",
       "      <td>0.902934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.256222</td>\n",
       "      <td>0.907474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>0.304459</td>\n",
       "      <td>0.907734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5 Training Time: 1383.28 seconds\n",
      "STEP 5 Accuracy: 0.9077\n"
     ]
    }
   ],
   "source": [
    "# same as above except using bucketing\n",
    "training_args_step5 = TrainingArguments(\n",
    "    output_dir=\"./results_step5\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    group_by_length=True,           # align by length\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# same as above\n",
    "trainer_step5 = Trainer(\n",
    "    model=get_model(),\n",
    "    args=training_args_step5,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# measure time spent in training the model\n",
    "print(\"\\n>>> STEP 5: Training WITH Bucketing...\")\n",
    "start_time_5 = time.time()\n",
    "train_result_5 = trainer_step5.train()\n",
    "end_time_5 = time.time()\n",
    "\n",
    "#save the results\n",
    "step5_time = end_time_5 - start_time_5\n",
    "step5_acc = trainer_step5.evaluate()['eval_accuracy']\n",
    "\n",
    "print(f\"STEP 5 Training Time: {step5_time:.2f} seconds\")\n",
    "print(f\"STEP 5 Accuracy: {step5_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQApvAjI17nV",
    "outputId": "e1ac16e0-d727-4af5-a414-766c030f058e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COMPARISON REPORT\n",
      "Standard Accuracy : 90.69%\n",
      "Bucketing Accuracy: 90.77%\n",
      "Accuracy Gap: 0.09%p\n",
      "Standard Time: 1988.23 sec\n",
      "Bucketing Time: 1383.28 sec\n",
      "Speed Improvement: 604.96 sec faster\n",
      "Validation accuracy > 90%\n",
      "Bucketting saved me some time for real\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL COMPARISON REPORT\")\n",
    "print(f\"Standard Accuracy : {step4_acc*100:.2f}%\")\n",
    "print(f\"Bucketing Accuracy: {step5_acc*100:.2f}%\")\n",
    "print(f\"Accuracy Gap: {(step5_acc - step4_acc)*100:.2f}%p\")\n",
    "print(f\"Standard Time: {step4_time:.2f} sec\")\n",
    "print(f\"Bucketing Time: {step5_time:.2f} sec\")\n",
    "print(f\"Speed Improvement: {step4_time - step5_time:.2f} sec faster\")\n",
    "\n",
    "if step4_acc >= 0.9:\n",
    "    print(\"Validation accuracy > 90%\")\n",
    "else:\n",
    "    print(\"Accuracy is less than 90%\")\n",
    "\n",
    "if step5_time < step4_time:\n",
    "    print(\"Bucketing saved me some time for real\")\n",
    "else:\n",
    "    print(\"No difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud68c\uace0:\n",
    "bucketing On/Off \ud558\ub098\ub85c \uc5f0\uc0b0\uc18d\ub3c4 \ucc28\uc774\uac00 \uc0c1\ub2f9\ud574\uc11c \uaf64 \ub180\ub77c\uc6e0\ub2e4.\n",
    "\ubaa8\ub378 \uc131\ub2a5 \ucc28\uc774\ub3c4 \ub9ce\uc774 \ub180\ub78d\uae34 \ud588\uc73c\ub098 (\uae30\uc874 \uc6b0\ub9ac\uac00 \uc9c4\ud589\ud55c \uac83\uc5d0 \ube44\ud574) \uc0ac\uc2e4 \uc6b0\ub9ac\uac00 \uc9c0\uae08 \ud558\ub294\uac74 \uc77c\ubc18\ud654 \ub2a5\ub825\uc774 \uc788\ub294 (=Pre-Trained) \ubaa8\ub378\uc744 \uac00\uc9c0\uace0 \uc6b0\ub9ac\uc758 \ud0dc\uc2a4\ud06c\uc5d0 \ub9de\ucda4\ud615\uc73c\ub85c \ubc14\uafb8\ub294 Fine-Tuning \uc774\ub77c\ub294 \uc810\uc744 \uac10\uc548\ud558\uba74 \uc5b4\ucc0c\ubcf4\uba74 \ub2f9\uc5f0\ud558\ub2e4\uace0 \uc0dd\uac01\ub41c\ub2e4. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}