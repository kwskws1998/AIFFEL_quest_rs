{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d805a05",
      "metadata": {},
      "source": [
        "1. 트랜스포머와 비교해 변경이 필요한 부분 서술\n",
        "- 인코더, 크로스 어텐션 등을 제거\n",
        "- 활성화 함수를 GELU로 교체\n",
        "- Postional encoding 을 Postional Embedding로, 즉 훈련시 학습가능한 임베딩 기반 위치정보 추가로 변경\n",
        "- Input Transformation을 위한 'sep' 추가"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63a4c608",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "31a5de5d",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d60a3ffa",
      "metadata": {
        "id": "d60a3ffa"
      },
      "source": [
        "## Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "lhCCPcWRKUU6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhCCPcWRKUU6",
        "outputId": "3f22f065-75cb-407c-aeda-9bf99b14e64e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# ===== 필요한 라이브러리 import =====\n",
        "\n",
        "# PyTorch 관련\n",
        "import torch  # PyTorch 메인 라이브러리\n",
        "import torch.nn as nn  # 신경망 모듈 (레이어, 활성화 함수 등)\n",
        "import torch.optim as optim  # 최적화 알고리즘 (Adam, SGD 등)\n",
        "import torch.nn.functional as F  # 함수형 API (softmax, relu 등)\n",
        "\n",
        "# 데이터 처리 관련\n",
        "import pandas as pd  # 데이터프레임 처리 (CSV 읽기 등)\n",
        "import numpy as np  # 수치 연산\n",
        "import re  # 정규표현식 (텍스트 전처리)\n",
        "import math  # 수학 함수 (sin, cos 등)\n",
        "\n",
        "# 토크나이저 및 데이터 로더\n",
        "import sentencepiece as spm  # SentencePiece 토크나이저 (서브워드 단위)\n",
        "from sklearn.model_selection import train_test_split  # 데이터를 train/val/test로 분할\n",
        "from torch.utils.data import Dataset, DataLoader  # PyTorch 데이터셋 및 배치 로더\n",
        "\n",
        "# 유틸리티\n",
        "from tqdm import tqdm  # 진행바 표시용\n",
        "import easydict  # 딕셔너리를 객체처럼 사용 가능 (config.emb_dim 형태)\n",
        "\n",
        "# 디바이스 설정: Apple Silicon의 MPS 가속기 사용 가능하면 사용, 아니면 CPU\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')  # 현재 사용 중인 디바이스 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c7115a",
      "metadata": {
        "id": "82c7115a"
      },
      "source": [
        "## Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ddHSeE00KUU8",
      "metadata": {
        "id": "ddHSeE00KUU8"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    \"\"\"\n",
        "    단일 문장을 전처리하는 함수\n",
        "    목적: 텍스트 데이터를 정제하여 모델 학습에 적합한 형태로 만듦\n",
        "    \"\"\"\n",
        "    # 입력 검증: 비어있거나 None인 경우 빈 문자열 반환\n",
        "    if pd.isna(sentence) or sentence is None:\n",
        "        return \"\"\n",
        "\n",
        "    # 문자열로 변환 (안전장치)\n",
        "    sentence = str(sentence)\n",
        "\n",
        "    # 정규표현식으로 필요한 문자만 남기기\n",
        "    # ㄱ-ㅎ: 자음, ㅏ-ㅣ: 모음, 가-힣: 완성형 한글\n",
        "    # a-zA-Z: 영어, 0-9: 숫자, \\s: 공백\n",
        "    # .,!?~: 문장부호, ㅠㅜ: 이모티콘\n",
        "    # 나머지는 모두 공백으로 치환\n",
        "    sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\s.,!?~ㅠㅜ]', ' ', sentence) #re.sub는 정규표현식(regex)을 사용하여 특정 패턴의 문자를 찾아 다른 문자로 바꿉니다.\n",
        "\n",
        "    # 연속된 여러 공백을 하나의 공백으로 통일\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    # 설명: \\s는 공백, +는 \"1개 이상\"을 의미합니다. 즉, \\s+는 \"1개 이상의 연속된 공백\"(예: \" \", \" \", \" \")을 찾습니다.\n",
        "\n",
        "    # 동작: 이렇게 찾은 연속된 공백 뭉치를 **하나의 공백(' ')**으로 압축합니다.\n",
        "\n",
        "    # 이유: 바로 앞 단계에서 Hi!!^^가 Hi 처럼 여러 공백으로 바뀔 수 있습니다. 단어 사이처럼 불필요하게 공백이 많은 것은 단어 사이와 동일하게 취급되어야 합니다.\n",
        "\n",
        "    # 문장 앞뒤 공백 제거\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    # 연속된 문장부호 정리 (예: !!! -> !, ??? -> ?)\n",
        "    # ([!?.])를 캡처하고 \\1+로 반복을 찾아서 r'\\1'로 하나만 남김\n",
        "    sentence = re.sub(r'([!?.])\\1+', r'\\1', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    CSV 파일에서 질문-답변 데이터를 로드하고 전처리\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"데이터 로드 및 전처리 중...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # pandas로 CSV 파일 읽기\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"전체 데이터: {len(df)} 쌍\")\n",
        "\n",
        "    # 전처리된 질문과 답변을 저장할 리스트 초기화\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    # 모든 질문-답변 쌍을 순회\n",
        "    for i, (q, a) in enumerate(zip(df['Q'], df['A'])):\n",
        "        # 각각 전처리 적용\n",
        "        clean_q = preprocess_sentence(q)\n",
        "        clean_a = preprocess_sentence(a)\n",
        "\n",
        "        # 둘 다 유효한 문장인 경우만 저장\n",
        "        if clean_q and clean_a:\n",
        "            questions.append(clean_q)\n",
        "            answers.append(clean_a)\n",
        "\n",
        "        # 진행 상황 출력 (매 1000개마다)\n",
        "        if (i + 1) % 1000 == 0:\n",
        "            print(f\"진행: {i + 1}/{len(df)}\")\n",
        "\n",
        "    print(f\"\\n전처리 후 유효한 쌍: {len(questions)}\")\n",
        "    print(\"\\n샘플 데이터:\")\n",
        "\n",
        "    # 처음 3개의 샘플 데이터 출력하여 확인\n",
        "    for i in range(min(3, len(questions))):\n",
        "        print(f\"Q: {questions[i]}\")\n",
        "        print(f\"A: {answers[i]}\\n\")\n",
        "\n",
        "    return questions, answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "T_yMit4sKUU8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_yMit4sKUU8",
        "outputId": "ae95833a-e0f9-4ff5-9d60-2b6079c5e282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "데이터 로드 및 전처리 중...\n",
            "==================================================\n",
            "전체 데이터: 11823 쌍\n",
            "진행: 1000/11823\n",
            "진행: 2000/11823\n",
            "진행: 3000/11823\n",
            "진행: 4000/11823\n",
            "진행: 5000/11823\n",
            "진행: 6000/11823\n",
            "진행: 7000/11823\n",
            "진행: 8000/11823\n",
            "진행: 9000/11823\n",
            "진행: 10000/11823\n",
            "진행: 11000/11823\n",
            "\n",
            "전처리 후 유효한 쌍: 11823\n",
            "\n",
            "샘플 데이터:\n",
            "Q: 12시 땡!\n",
            "A: 하루가 또 가네요.\n",
            "\n",
            "Q: 1지망 학교 떨어졌어\n",
            "A: 위로해 드립니다.\n",
            "\n",
            "Q: 3박4일 놀러가고 싶다\n",
            "A: 여행은 언제나 좋죠.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 데이터 로드 및 전처리 실행\n",
        "# 파일 경로를 실제 데이터가 있는 위치로 수정 필요\n",
        "file_path = '/Users/wansookim/Downloads/code_implementation/transformer_project_submit/data/ChatbotData.csv'\n",
        "\n",
        "# 함수 호출하여 전처리된 질문과 답변 리스트 받기\n",
        "questions, answers = load_and_preprocess_data(file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dfc612",
      "metadata": {
        "id": "d5dfc612"
      },
      "source": [
        "## Step 3: SentencePiece Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "93XtZLrhKUU8",
      "metadata": {
        "id": "93XtZLrhKUU8"
      },
      "outputs": [],
      "source": [
        "def train_sentencepiece_model(questions, answers, model_prefix='/Users/wansookim/Downloads/code_implementation/transformer_project_submit', vocab_size=1200):\n",
        "    \"\"\"\n",
        "    SentencePiece 모델 학습\n",
        "    SentencePiece: 텍스트를 서브워드 단위로 분리하는 토크나이저\n",
        "    장점: OOV(Out-of-Vocabulary) 문제 해결, 한국어에 효과적\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"SentencePiece 모델 학습 중...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 모든 문장을 하나의 텍스트 파일로 저장 (SentencePiece 입력 형식)\n",
        "    all_sentences_path = '/Users/wansookim/Downloads/code_implementation/transformer_project_submit/sentencepiece'\n",
        "    with open(all_sentences_path, 'w', encoding='utf-8') as f:\n",
        "        # 질문과 답변을 모두 합쳐서 줄바꿈으로 구분하여 저장\n",
        "        f.write('\\n'.join(questions + answers))\n",
        "\n",
        "    # SentencePiece 학습 명령어 설정\n",
        "    cmd = f'--input={all_sentences_path} \\\n",
        "           --model_prefix={model_prefix} \\\n",
        "           --vocab_size={vocab_size} \\\n",
        "           --model_type=unigram \\\n",
        "           --max_sentence_length=999999 \\\n",
        "           --pad_id=0 \\\n",
        "           --unk_id=1 \\\n",
        "           --bos_id=2 \\\n",
        "           --eos_id=3 \\\n",
        "           --user_defined_symbols=[SEP],[CLS],[MASK]'\n",
        "    # --input: 학습 데이터 경로\n",
        "    # --model_prefix: 저장될 모델 파일명 접두사\n",
        "    # --vocab_size: 어휘 사전 크기 (8000개의 서브워드)\n",
        "    # --model_type: unigram 언어 모델 사용\n",
        "    # --pad_id: 패딩 토큰 ID (0)\n",
        "    # --unk_id: 미등록 토큰 ID (1)\n",
        "    # --bos_id: 문장 시작 토큰 ID (2)\n",
        "    # --eos_id: 문장 종료 토큰 ID (3)\n",
        "\n",
        "    # SentencePiece 모델 학습 실행\n",
        "    spm.SentencePieceTrainer.Train(cmd)\n",
        "\n",
        "    # 학습된 모델 파일 경로 생성\n",
        "    model_file = f\"{model_prefix}.model\"\n",
        "    print(f\"\\n모델 저장됨: {model_file}\")\n",
        "    return model_file\n",
        "\n",
        "\n",
        "class SentencePieceVocab:\n",
        "    \"\"\"\n",
        "    SentencePiece 모델 래퍼 클래스\n",
        "    목적: SentencePiece 모델을 쉽게 사용하기 위한 인터페이스 제공\n",
        "    \"\"\"\n",
        "    def __init__(self, sp_model_path):\n",
        "        # SentencePiece 프로세서 초기화\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        # 학습된 모델 로드\n",
        "        self.sp.Load(sp_model_path)\n",
        "\n",
        "        # 특수 토큰 ID 정의\n",
        "        self.PAD_ID = 0  # 패딩 (빈 공간 채우기)\n",
        "        self.UNK_ID = 1  # 미등록 단어\n",
        "        self.BOS_ID = 2  # 문장 시작 (Beginning Of Sentence)\n",
        "        self.EOS_ID = 3  # 문장 끝 (End Of Sentence)\n",
        "        self.SEP_ID = 4\n",
        "\n",
        "        # 토큰 문자열 -> ID 매핑\n",
        "        self.stoi = {'<pad>': 0, '<unk>': 1, '<s>': 2, '</s>': 3, '[SEP]': 4}\n",
        "\n",
        "        # ID -> 토큰 문자열 매핑 (전체 어휘)\n",
        "        self.itos = [self.sp.IdToPiece(i) for i in range(self.sp.GetPieceSize())]\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        \"\"\"문장을 토큰 ID 리스트로 인코딩\"\"\"\n",
        "        return self.sp.EncodeAsIds(sentence)\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        토큰 ID 리스트를 문장으로 디코딩\n",
        "        특수 토큰(pad, bos, eos)은 제외하고 디코딩\n",
        "        \"\"\"\n",
        "        return self.sp.DecodeIds([i for i in ids if i not in [0, 2, 3]])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"어휘 사전 크기 반환\"\"\"\n",
        "        return self.sp.GetPieceSize()\n",
        "\n",
        "\n",
        "class ChatbotDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset 클래스\n",
        "    목적: 질문-답변 쌍을 PyTorch 모델에 입력 가능한 형태로 변환\n",
        "    \"\"\"\n",
        "    def __init__(self, questions, answers, vocab, max_length=40):\n",
        "        # 데이터 저장\n",
        "        self.vocab = vocab  # SentencePiece vocab 객체\n",
        "        self.max_length = max_length  # 최대 시퀀스 길이 (잘림 방지)\n",
        "        self.sequences = []\n",
        "\n",
        "        #모든 질문 답변 쌍을 시퀀스로 합쳐버리기\n",
        "        for q, a in zip(questions, answers):\n",
        "            sequence = (  [self.vocab.BOS_ID] + self.vocab.encode(q) + [self.vocab.SEP_ID] +  self.vocab.encode(a) + [self.vocab.EOS_ID])\n",
        "        \n",
        "            if len(sequence) > max_length:\n",
        "                sequence = sequence[:max_length]\n",
        "            else: \n",
        "                pad_length = max_length - len(sequence)\n",
        "                sequence = sequence + [self.vocab.PAD_ID] * pad_length\n",
        "            self.sequences.append(sequence)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"데이터셋 크기 반환\"\"\"\n",
        "        return len(self.sequences)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "    Shifted sequences 반환\n",
        "        - input_ids:  [BOS, w1, w2, ..., wN]\n",
        "        - target_ids: [w1, w2, ..., wN, EOS]\n",
        "        \"\"\"\n",
        "        sequence = self.sequences[idx]\n",
        "        tokens = torch.tensor(sequence, dtype=torch.long)\n",
        "        # Next-token prediction을 위한 shifted sequences\n",
        "        input_ids = tokens[:-1]   # 마지막 토큰 제외\n",
        "        target_ids = tokens[1:]   # 첫 토큰 제외\n",
        "        \n",
        "        return {\n",
        "            'input_ids': input_ids,    # ← SRC 대신\n",
        "            'target_ids': target_ids   # ← TRG 대신\n",
        "        }       \n",
        "\n",
        "\n",
        "def collate_fn(batch, pad_idx=0):\n",
        "    \"\"\"\n",
        "    DataLoader의 배치 생성 함수\n",
        "    목적: 서로 다른 길이의 시퀀스를 같은 길이로 패딩하여 배치 생성\n",
        "    \"\"\"\n",
        "    # 배치에서 SRC(질문)와 TRG(답변) 분리\n",
        "    input_batch = [item['input_ids'] for item in batch]\n",
        "    target_batch = [item['target_ids'] for item in batch]\n",
        "\n",
        "    # 리스트의 텐서들을 하나의 텐서로 쌓기 (batch_size, seq_len)\n",
        "    return {'input_ids': torch.stack(input_batch), \n",
        "            'target_ids': torch.stack(target_batch)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "X4ScpHrOKUU9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4ScpHrOKUU9",
        "outputId": "c9b7e97b-755b-4f68-b710-8c113268b747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "SentencePiece 모델 학습 중...\n",
            "==================================================\n",
            "\n",
            "모델 저장됨: /Users/wansookim/Downloads/code_implementation/transformer_project_submit.model\n",
            "\n",
            "어휘 사전 크기: 1,200\n",
            "\n",
            "테스트 문장: 12시 땡!\n",
            "인코딩 결과 (처음 10개): [7, 680, 311, 55, 7, 1022, 115]...\n",
            "디코딩 결과: 12시 땡!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/Users/wansookim/Downloads/code_implementation/transformer_project_submit/sentencepiece            --model_prefix=/Users/wansookim/Downloads/code_implementation/transformer_project_submit            --vocab_size=1200            --model_type=unigram            --max_sentence_length=999999            --pad_id=0            --unk_id=1            --bos_id=2            --eos_id=3            --user_defined_symbols=[SEP],[CLS],[MASK]\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: /Users/wansookim/Downloads/code_implementation/transformer_project_submit/sentencepiece\n",
            "  input_format: \n",
            "  model_prefix: /Users/wansookim/Downloads/code_implementation/transformer_project_submit\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 1200\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 999999\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: [SEP]\n",
            "  user_defined_symbols: [CLS]\n",
            "  user_defined_symbols: [MASK]\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 1\n",
            "  bos_id: 2\n",
            "  eos_id: 3\n",
            "  pad_id: 0\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(186) LOG(INFO) Loading corpus: /Users/wansookim/Downloads/code_implementation/transformer_project_submit/sentencepiece\n",
            "trainer_interface.cc(411) LOG(INFO) Loaded all 23646 sentences\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [SEP]\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [CLS]\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [MASK]\n",
            "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(541) LOG(INFO) all chars count=353346\n",
            "trainer_interface.cc(552) LOG(INFO) Done: 99.9505% characters are covered.\n",
            "trainer_interface.cc(562) LOG(INFO) Alphabet size=1081\n",
            "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999505\n",
            "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 23646 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=162511\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 18854 seed sentencepieces\n",
            "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 23646\n",
            "trainer_interface.cc(611) LOG(INFO) Done! 21767\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 21767 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10901 obj=13.2232 num_tokens=46216 num_tokens/piece=4.23961\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9585 obj=12.117 num_tokens=46316 num_tokens/piece=4.83213\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7188 obj=12.4735 num_tokens=49348 num_tokens/piece=6.86533\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7187 obj=12.417 num_tokens=49382 num_tokens/piece=6.87102\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5390 obj=12.9323 num_tokens=53504 num_tokens/piece=9.92653\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5390 obj=12.8517 num_tokens=53504 num_tokens/piece=9.92653\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4042 obj=13.5925 num_tokens=57912 num_tokens/piece=14.3276\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4042 obj=13.4963 num_tokens=57922 num_tokens/piece=14.33\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3031 obj=14.2864 num_tokens=62707 num_tokens/piece=20.6886\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3031 obj=14.1763 num_tokens=62712 num_tokens/piece=20.6902\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2273 obj=15.0025 num_tokens=67901 num_tokens/piece=29.8729\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2273 obj=14.87 num_tokens=67903 num_tokens/piece=29.8737\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1704 obj=15.9206 num_tokens=73528 num_tokens/piece=43.1502\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1704 obj=15.745 num_tokens=73684 num_tokens/piece=43.2418\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1320 obj=16.9886 num_tokens=80028 num_tokens/piece=60.6273\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1320 obj=16.756 num_tokens=80029 num_tokens/piece=60.628\n",
            "trainer_interface.cc(689) LOG(INFO) Saving model: /Users/wansookim/Downloads/code_implementation/transformer_project_submit.model\n",
            "trainer_interface.cc(701) LOG(INFO) Saving vocabs: /Users/wansookim/Downloads/code_implementation/transformer_project_submit.vocab\n"
          ]
        }
      ],
      "source": [
        "# ===== SentencePiece 모델 학습 및 어휘 사전 생성 =====\n",
        "\n",
        "# SentencePiece 모델 학습 실행\n",
        "model_file = train_sentencepiece_model(questions, answers)\n",
        "\n",
        "# Vocab 객체 생성 (토크나이저 래퍼)\n",
        "vocab = SentencePieceVocab(model_file)\n",
        "print(f\"\\n어휘 사전 크기: {len(vocab):,}\")  # 어휘에 포함된 총 토큰 수\n",
        "\n",
        "# 토크나이징 테스트\n",
        "test_sentence = questions[0]  # 첫 번째 질문으로 테스트\n",
        "encoded = vocab.encode(test_sentence)  # 문장 -> 토큰 ID로 인코딩\n",
        "decoded = vocab.decode(encoded)  # 토큰 ID -> 문장으로 디코딩\n",
        "\n",
        "# 인코딩/디코딩 결과 확인\n",
        "print(f\"\\n테스트 문장: {test_sentence}\")\n",
        "print(f\"인코딩 결과 (처음 10개): {encoded[:10]}...\")  # 토큰 ID 리스트\n",
        "print(f\"디코딩 결과: {decoded}\")  # 다시 문장으로 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "wJmxylqUKUU9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJmxylqUKUU9",
        "outputId": "0702fda6-5c7d-4cc7-a792-1adc0ff37e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "데이터 분할 완료\n",
            "==================================================\n",
            "Train: 9,458 쌍\n",
            "Val: 1,182 쌍\n",
            "Test: 1,183 쌍\n"
          ]
        }
      ],
      "source": [
        "# ===== 데이터 분할: Train / Validation / Test =====\n",
        "\n",
        "# 1단계: 전체 데이터를 Train(80%) + Temp(20%)로 분할\n",
        "train_q, temp_q, train_a, temp_a = train_test_split(\n",
        "    questions, answers,  # 전체 데이터\n",
        "    test_size=0.2,  # 20%를 temp로\n",
        "    random_state=42  # 재현성을 위한 랜덤 시드\n",
        ")\n",
        "\n",
        "# 2단계: Temp 데이터를 Validation(10%) + Test(10%)로 분할\n",
        "val_q, test_q, val_a, test_a = train_test_split(\n",
        "    temp_q, temp_a,  # temp 데이터 (전체의 20%)\n",
        "    test_size=0.5,  # temp의 50% = 전체의 10%\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# PyTorch Dataset 객체 생성\n",
        "train_dataset = ChatbotDataset(train_q, train_a, vocab, max_length=40)\n",
        "val_dataset = ChatbotDataset(val_q, val_a, vocab, max_length=40)\n",
        "test_dataset = ChatbotDataset(test_q, test_a, vocab, max_length=40)\n",
        "\n",
        "# DataLoader 생성 (배치 단위로 데이터 로드)\n",
        "train_iterator = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,  # 한 번에 32개씩 처리\n",
        "    shuffle=True,  # 학습 시 데이터 섞기 (과적합 방지)\n",
        "    collate_fn=lambda b: collate_fn(b, vocab.PAD_ID)  # 패딩 적용\n",
        ")\n",
        "\n",
        "valid_iterator = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,  # 검증 시에는 섞지 않음\n",
        "    collate_fn=lambda b: collate_fn(b, vocab.PAD_ID)\n",
        ")\n",
        "\n",
        "test_iterator = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,  # 테스트 시에도 섞지 않음\n",
        "    collate_fn=lambda b: collate_fn(b, vocab.PAD_ID)\n",
        ")\n",
        "\n",
        "# 데이터 분할 결과 출력\n",
        "print(\"=\" * 50)\n",
        "print(\"데이터 분할 완료\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Train: {len(train_q):,} 쌍\")  # 학습용\n",
        "print(f\"Val: {len(val_q):,} 쌍\")  # 검증용 (하이퍼파라미터 튜닝)\n",
        "print(f\"Test: {len(test_q):,} 쌍\")  # 최종 평가용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95eb9a6",
      "metadata": {
        "id": "d95eb9a6"
      },
      "source": [
        "## Step 4: Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "BsSLJeUZKUU9",
      "metadata": {
        "id": "BsSLJeUZKUU9"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    멀티 헤드 어텐션 (Multi-Head Attention)\n",
        "    논문: 'Attention Is All You Need' (Vaswani et al., 2017)\n",
        "\n",
        "    핵심 아이디어:\n",
        "    - 여러 개의 어텐션을 병렬로 수행 (다양한 관점에서 정보 추출)\n",
        "    - 각 헤드는 서로 다른 표현 부분공간에 집중\n",
        "    - 모든 헤드의 출력을 합쳐서 최종 출력 생성\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_dim, num_heads, dropout=0.0, bias=False, causal=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # 하이퍼파라미터 저장\n",
        "        self.emb_dim = emb_dim  # 임베딩 차원 (전체 모델의 차원)\n",
        "        self.num_heads = num_heads  # 어텐션 헤드 개수\n",
        "        self.dropout = dropout  # 드롭아웃 비율\n",
        "        self.head_dim = emb_dim // num_heads  # 각 헤드의 차원\n",
        "\n",
        "        # emb_dim이 num_heads로 나누어 떨어지는지 확인\n",
        "        # 예: emb_dim=64, num_heads=8 → head_dim=8 (OK)\n",
        "        assert self.head_dim * num_heads == self.emb_dim, \"emb_dim은 num_heads로 나누어떨어져야 함\"\n",
        "\n",
        "        # 어텐션 타입 설정\n",
        "        self.causal = causal  # 인과적 마스킹 여부 (미래 토큰 참조 금지)\n",
        "\n",
        "        # Query, Key, Value 투영 레이어\n",
        "        # 입력을 Q, K, V로 변환하는 선형 변환\n",
        "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=bias)  # Query 투영\n",
        "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=bias)  # Key 투영\n",
        "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=bias)  # Value 투영\n",
        "\n",
        "        # 최종 출력 투영 레이어\n",
        "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        \"\"\"\n",
        "        텐서를 멀티 헤드 어텐션 계산에 적합한 형태로 변환\n",
        "\n",
        "        입력 형태: (batch_size, seq_len, emb_dim)\n",
        "        출력 형태: (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        이렇게 하면 각 헤드가 독립적으로 어텐션을 계산할 수 있음\n",
        "        \"\"\"\n",
        "        # (batch_size, seq_len, emb_dim) → (batch_size, seq_len, num_heads, head_dim)\n",
        "        new_x_shape = x.size()[:-1] + (self.num_heads, self.head_dim,)\n",
        "        x = x.view(*new_x_shape)\n",
        "\n",
        "        # (batch_size, seq_len, num_heads, head_dim) → (batch_size, num_heads, seq_len, head_dim)\n",
        "        # permute로 헤드 차원을 앞으로 이동\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, query, key, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass: 어텐션 메커니즘 수행\n",
        "\n",
        "        Args:\n",
        "            query: 쿼리 텐서 (무엇을 찾고 싶은지)\n",
        "            key: 키/밸류 텐서 (어디서 정보를 가져올지)\n",
        "            attention_mask: 어텐션 마스크 (특정 위치 참조 금지)\n",
        "        \"\"\"\n",
        "        # Query 투영: (batch, seq_len, emb_dim)\n",
        "        # 셀프 어텐션: Query, Key, Value 모두 같은 입력에서\n",
        "        q = self.q_proj(query)\n",
        "        k = self.k_proj(query)\n",
        "        v = self.v_proj(query)\n",
        "\n",
        "        # 멀티 헤드 형태로 변환\n",
        "        q = self.transpose_for_scores(q)  # (batch, num_heads, seq_len, head_dim)\n",
        "        k = self.transpose_for_scores(k)\n",
        "        v = self.transpose_for_scores(v)\n",
        "\n",
        "        # 어텐션 스코어 계산: Q와 K의 내적\n",
        "        # (batch, num_heads, seq_len_q, head_dim) @ (batch, num_heads, head_dim, seq_len_k)\n",
        "        # → (batch, num_heads, seq_len_q, seq_len_k)\n",
        "        attn_weights = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "        # sqrt(head_dim)으로 나누는 이유: 스케일 조정 (그래디언트 안정화)\n",
        "\n",
        "        # 어텐션 마스크 적용 (필요한 경우)\n",
        "        if attention_mask is not None:\n",
        "            if self.causal:\n",
        "                # 인과적 마스킹: 미래 토큰을 볼 수 없도록 (디코더 self-attention)\n",
        "                # -inf로 설정하면 softmax 후 0이 됨\n",
        "                attn_weights = attn_weights.masked_fill(\n",
        "                    attention_mask.unsqueeze(0).unsqueeze(1), float(\"-inf\")\n",
        "                )\n",
        "            # else:\n",
        "            #     # 패딩 마스킹: 패딩 토큰을 참조하지 않도록\n",
        "            #     attn_weights = attn_weights.masked_fill(\n",
        "            #         attention_mask.unsqueeze(1).unsqueeze(2), float(\"-inf\")\n",
        "            #     )\n",
        "\n",
        "        # Softmax로 어텐션 확률 계산 (각 위치에 얼마나 집중할지)\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)  # 합이 1이 되도록\n",
        "\n",
        "        # 드롭아웃 적용 (학습 시 정규화)\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        # 어텐션 가중치와 Value의 가중합 계산\n",
        "        # (batch, num_heads, seq_len, seq_len) @ (batch, num_heads, seq_len, head_dim)\n",
        "        # → (batch, num_heads, seq_len, head_dim)\n",
        "        attn_output = torch.matmul(attn_probs, v)\n",
        "\n",
        "        # 원래 형태로 되돌리기\n",
        "        # (batch, num_heads, seq_len, head_dim) → (batch, seq_len, num_heads, head_dim)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # 모든 헤드를 연결 (concatenate)\n",
        "        # (batch, seq_len, num_heads, head_dim) → (batch, seq_len, emb_dim)\n",
        "        concat_attn_output_shape = attn_output.size()[:-2] + (self.emb_dim,)\n",
        "        attn_output = attn_output.view(*concat_attn_output_shape)\n",
        "\n",
        "        # 최종 선형 투영\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        # 출력: 어텐션 결과와 어텐션 가중치 (시각화용)\n",
        "        return attn_output, attn_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "Ic0RsbXFKUU-",
      "metadata": {
        "id": "Ic0RsbXFKUU-"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    위치별 피드포워드 네트워크 (Position-wise Feed-Forward Network)\n",
        "\n",
        "    Transformer의 각 위치(토큰)에 독립적으로 적용되는 2층 fully-connected 네트워크\n",
        "    역할: 어텐션으로 모은 정보를 비선형 변환하여 더 풍부한 표현 학습\n",
        "\n",
        "    구조: Linear → ReLU → Dropout → Linear → Dropout → Residual Connection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_dim, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # 첫 번째 선형 레이어: emb_dim → d_ff (차원 확장)\n",
        "        # 보통 d_ff = 4 * emb_dim (예: 64 → 256)\n",
        "        self.w_1 = nn.Linear(emb_dim, d_ff)\n",
        "\n",
        "        # 두 번째 선형 레이어: d_ff → emb_dim (차원 축소)\n",
        "        self.w_2 = nn.Linear(d_ff, emb_dim)\n",
        "\n",
        "        self.dropout = dropout  # 드롭아웃 비율\n",
        "        self.activation = nn.GELU()  # 활성화 함수 (비선형성 추가)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        입력: (batch_size, seq_len, emb_dim)\n",
        "        출력: (batch_size, seq_len, emb_dim)\n",
        "        \"\"\"\n",
        "        # Residual connection을 위해 입력 저장\n",
        "        residual = x\n",
        "\n",
        "        # 1. 차원 확장 및 활성화\n",
        "        x = self.activation(self.w_1(x))  # (batch, seq_len, emb_dim) → (batch, seq_len, d_ff)\n",
        "\n",
        "        # 2. 드롭아웃 (학습 시 정규화)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # 3. 차원 축소 (원래 크기로)\n",
        "        x = self.w_2(x)  # (batch, seq_len, d_ff) → (batch, seq_len, emb_dim)\n",
        "\n",
        "        # 4. 드롭아웃\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # 5. Residual connection (입력을 출력에 더함)\n",
        "        # 이유: 그래디언트 소실 방지, 학습 안정화\n",
        "        return x + residual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ErN_GrmvKUU-",
      "metadata": {
        "id": "ErN_GrmvKUU-"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "GPT-1 논문에서 제안된 위치 임베딩 (Positional Embedding)\n",
        "훈련중에 업데이트 되는 패러미터 값들로 위치 값 설정됨\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_positions: 최대 위치 개수 (최대 시퀀스 길이)\n",
        "            embedding_dim: 임베딩 차원\n",
        "            padding_idx: 패딩 인덱스 (사용 안 함)\n",
        "        \"\"\"\n",
        "        # 부모 클래스(nn.Embedding) 초기화\n",
        "        super().__init__(num_positions, embedding_dim, padding_idx)\n",
        "\n",
        "        # weight 파라미터를 정규분포 통해서 초기화\n",
        "        nn.init.normal_(self.weight, mean=0.0, std=0.01)\n",
        "        #학습되야 하니까 requires_grad = True로 둬야한다\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Forward pass: 입력 시퀀스 길이에 맞는 위치 임베딩 반환\n",
        "\n",
        "        Args:\n",
        "            input_ids: 입력 토큰 ID (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            위치 임베딩: (seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        bsz, seq_len = input_ids.shape[:2]\n",
        "        \n",
        "        # Create position indices [0, 1, 2, ..., seq_len-1]\n",
        "        positions = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)\n",
        "        \n",
        "        # Look up learned embeddings (same as nn.Embedding.forward)\n",
        "        return super().forward(positions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55316d11",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "af-HTCxuKUU-",
      "metadata": {
        "id": "af-HTCxuKUU-"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer 디코더의 단일 레이어\n",
        "\n",
        "    구조 (3개의 서브레이어):\n",
        "    1. Masked Multi-Head Self-Attention (미래 토큰 참조 불가)\n",
        "    2. Multi-Head Cross-Attention (인코더 출력 참조)\n",
        "    3. Position-wise Feed-Forward Network\n",
        "\n",
        "    각 서브레이어마다 residual connection + layer normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Masked Self-Attention\n",
        "        # 디코더의 각 위치는 이전 위치들만 참조 가능 (causal=True)\n",
        "        self.self_attn = MultiHeadAttention(\n",
        "            emb_dim=config.emb_dim,\n",
        "            num_heads=config.num_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            causal=True  # 미래 토큰 마스킹\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(config.emb_dim)\n",
        "\n",
        "        # 2. Feed-Forward Network\n",
        "        self.ffn = PositionWiseFeedForward(\n",
        "            emb_dim=config.emb_dim,\n",
        "            d_ff=config.ffn_dim,\n",
        "            dropout=config.dropout\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(config.emb_dim)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "    def forward(self, x, decoder_causal_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            x: 디코더 입력 (batch_size, tgt_len, emb_dim)\n",
        "            decoder_causal_mask: 디코더 인과 마스크 (미래 마스킹)\n",
        "        \"\"\"\n",
        "        # 1. Masked Self-Attention\n",
        "        # 현재까지 생성된 토큰들끼리만 어텐션\n",
        "        self_attn_output, self_attn_weights = self.self_attn(\n",
        "            query=x,\n",
        "            key=x,  # 셀프 어텐션\n",
        "            attention_mask=decoder_causal_mask  # 미래 마스킹\n",
        "        )\n",
        "\n",
        "        # Residual + Dropout + LayerNorm\n",
        "        x = x + F.dropout(self_attn_output, p=self.dropout, training=self.training)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "\n",
        "        # 2. Feed-Forward Network\n",
        "        x = self.ffn(x)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # 출력과 어텐션 가중치 반환\n",
        "        return x, (self_attn_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "SvHC5_xaKUU_",
      "metadata": {
        "id": "SvHC5_xaKUU_"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer 디코더\n",
        "\n",
        "    역할:\n",
        "    - 인코더의 출력을 참조하면서 순차적으로 답변 생성\n",
        "    - 이전에 생성한 토큰들과 인코더 정보를 활용\n",
        "\n",
        "    처리 과정:\n",
        "    1. 타겟 토큰 임베딩 + 위치 임베딩\n",
        "    2. N개의 DecoderLayer 통과\n",
        "    3. 다음 토큰 예측을 위한 표현 출력\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, embed_tokens):\n",
        "        super().__init__()\n",
        "\n",
        "        # 패딩 인덱스\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "\n",
        "        # 타겟 토큰 임베딩 (답변 단어 -> 벡터)\n",
        "        self.embed_tokens = embed_tokens\n",
        "\n",
        "        # 위치 임베딩\n",
        "        self.embed_positions = PositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            config.emb_dim,\n",
        "            self.padding_idx\n",
        "        )\n",
        "\n",
        "        # N개의 DecoderLayer 스택\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(config) for _ in range(config.decoder_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input_ids, decoder_causal_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            input_ids: 타겟 토큰 ID (batch_size, tgt_len)\n",
        "            decoder_causal_mask: 디코더 인과 마스크\n",
        "\n",
        "        Returns:\n",
        "            디코더 출력 (batch_size, tgt_len, emb_dim)\n",
        "            어텐션 스코어 리스트\n",
        "        \"\"\"\n",
        "        # 1. 타겟 토큰 임베딩\n",
        "        inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "        # 2. 위치 임베딩\n",
        "        embed_pos = self.embed_positions(input_ids)\n",
        "\n",
        "        # 3. 토큰 임베딩 + 위치 임베딩\n",
        "        x = inputs_embeds + embed_pos\n",
        "\n",
        "        # 어텐션 스코어 저장\n",
        "        attention_scores = []\n",
        "\n",
        "        # 4. 모든 DecoderLayer 통과\n",
        "        for layer in self.layers:\n",
        "            # 각 레이어는 셀프 어텐션 + 크로스 어텐션 + FFN 수행\n",
        "            x, attn = layer(\n",
        "                x,  # 디코더 입력\n",
        "                decoder_causal_mask=decoder_causal_mask\n",
        "            )\n",
        "            attention_scores.append(attn)  # (self_attn, cross_attn) 튜플\n",
        "\n",
        "        return x, attention_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a8_zWx3-KUU_",
      "metadata": {
        "id": "a8_zWx3-KUU_"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    완전한 Transformer 모델 (Sequence-to-Sequence)\n",
        "\n",
        "    구조:\n",
        "    1. 소스/타겟 임베딩 레이어\n",
        "    2. Encoder (입력 인코딩)\n",
        "    3. Decoder (출력 생성)\n",
        "    4. 최종 출력 레이어 (어휘 확률 분포)\n",
        "\n",
        "    사용 예:\n",
        "    - 기계 번역 (영어 -> 한국어)\n",
        "    - 대화 시스템 (질문 -> 답변)\n",
        "    - 요약, 생성 등\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # 설정 저장\n",
        "        self.vocab = vocab  # 어휘 사전\n",
        "        self.config = config\n",
        "\n",
        "        # 타겟(출력) 임베딩 레이어\n",
        "        self.dec_embedding = nn.Embedding(\n",
        "            len(vocab.itos), config.emb_dim, padding_idx=vocab.stoi['<pad>']\n",
        "        )\n",
        "\n",
        "        # Decoder 초기화\n",
        "        self.decoder = Decoder(config, self.dec_embedding)\n",
        "\n",
        "        # 출력 레이어: 임베딩 차원 -> 어휘 크기\n",
        "        # 각 위치에서 다음 토큰의 확률 분포 생성\n",
        "        self.prediction_head = nn.Linear(config.emb_dim, len(vocab.itos))\n",
        "\n",
        "    def generate_mask(self, trg):\n",
        "        \"\"\"\n",
        "        어텐션 마스크 생성\n",
        "\n",
        "        1. 디코더 인과 마스크: 미래 토큰 참조 방지\n",
        "\n",
        "        Args:\n",
        "            trg: 타겟 토큰 ID (batch_size, tgt_len)\n",
        "        \"\"\"\n",
        "        # 1. 디코더 인과 마스크 (Causal Mask)\n",
        "        # 각 위치에서 이후 위치를 볼 수 없도록 상삼각 행렬 생성\n",
        "        tgt_len = trg.size(1)\n",
        "        # torch.triu: 상삼각 행렬 (대각선 위쪽만 1)\n",
        "        # diagonal=1: 대각선 바로 위부터\n",
        "        # 예: [[0, 1, 1],\n",
        "        #      [0, 0, 1],\n",
        "        #      [0, 0, 0]]\n",
        "        dec_causal_mask = torch.triu(\n",
        "            torch.ones(tgt_len, tgt_len, dtype=torch.bool, device=trg.device),\n",
        "            diagonal=1\n",
        "        )\n",
        "        # True인 위치는 참조 불가 (미래 토큰)\n",
        "\n",
        "        return dec_causal_mask\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Forward pass: 소스에서 타겟으로 변환\n",
        "\n",
        "        Args:\n",
        "            trg: 타겟 시퀀스 (batch_size, tgt_len) - 답변\n",
        "\n",
        "        Returns:\n",
        "            output: 예측 로짓 (batch_size, tgt_len, vocab_size)\n",
        "            decoder_attention_scores: 디코더 어텐션\n",
        "        \"\"\"\n",
        "        # 1. 마스크 생성\n",
        "        dec_causal_mask = self.generate_mask(input_ids)\n",
        "\n",
        "        # 2. 디코더: 타겟 문장 생성\n",
        "        # 인코더 출력을 참조하면서 답변 생성\n",
        "        decoder_output, decoder_attention_scores = self.decoder(\n",
        "            input_ids,  # 타겟 입력 (teacher forcing)\n",
        "            decoder_causal_mask=dec_causal_mask,  # 미래 마스킹\n",
        "        )\n",
        "\n",
        "        # 4. 최종 출력: 어휘 확률 분포\n",
        "        # (batch_size, tgt_len, emb_dim) → (batch_size, tgt_len, vocab_size)\n",
        "        decoder_output = self.prediction_head(decoder_output)\n",
        "\n",
        "        return decoder_output, decoder_attention_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c4ZxXsD4KUU_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4ZxXsD4KUU_",
        "outputId": "5a7f84aa-b681-4514-dde7-ee8a9a722ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델 파라미터 수: 2,992,048\n"
          ]
        }
      ],
      "source": [
        "# ===== 모델 설정 및 초기화 =====\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "config = easydict.EasyDict({\n",
        "    # 모델 차원\n",
        "    \"emb_dim\": 256,  # 임베딩 차원 (단어를 64차원 벡터로 표현)\n",
        "    \"ffn_dim\": 1024,  # Feed-Forward 중간 차원 (보통 emb_dim의 4배)\n",
        "\n",
        "    # 어텐션 설정\n",
        "    \"num_heads\": 8,  # 멀티 헤드 어텐션의 헤드 개수\n",
        "    \"attention_dropout\": 0.1,  # 어텐션 가중치 드롭아웃\n",
        "\n",
        "    # 레이어 개수\n",
        "    \"decoder_layers\": 3,  # 디코더 레이어 수\n",
        "\n",
        "    # 기타\n",
        "    \"dropout\": 0.35,  # 일반 드롭아웃 (과적합 방지)\n",
        "    \"max_position_embeddings\": 40  # 최대 시퀀스 길이\n",
        "})\n",
        "\n",
        "# 모델 생성\n",
        "model = Transformer(vocab, config)\n",
        "\n",
        "# 모델을 디바이스로 이동 (GPU/MPS 사용 가능하면 사용)\n",
        "model.to(device)\n",
        "\n",
        "# 옵티마이저: Adam (학습률 0.001)\n",
        "# Adam은 학습률을 자동으로 조정하는 효율적인 최적화 알고리즘\n",
        "# optimizer = optim.AdamW(\n",
        "#     model.parameters(),\n",
        "#     lr=0.0005,\n",
        "#     betas=(0.9, 0.98),      # Standard for Transformers\n",
        "#     eps=1e-9,\n",
        "#     weight_decay=0.01       # Proper weight decay\n",
        "# )\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0005,          # 0.0005 → 0.0003\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9,\n",
        "    weight_decay=0.01    # 0.01 → 0.1\n",
        ")\n",
        "\n",
        "# 손실 함수: Cross Entropy Loss\n",
        "# ignore_index: 패딩 토큰은 손실 계산에서 제외\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])\n",
        "\n",
        "# 그래디언트 클리핑 값 (그래디언트 폭발 방지)\n",
        "CLIP = 1.0\n",
        "\n",
        "# 학습 에폭 수\n",
        "N_EPOCHS = 200\n",
        "\n",
        "# Early Stopping 설정\n",
        "best_valid_loss = float('inf')  # 최고 검증 손실 초기화\n",
        "patience = 5  # 성능 개선이 없으면 3 에폭 후 조기 종료\n",
        "patience_counter = 0  # 카운터 초기화\n",
        "\n",
        "print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b771c946",
      "metadata": {
        "id": "b771c946"
      },
      "source": [
        "## Step 5: Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b49BeGvQKUU_",
      "metadata": {
        "id": "b49BeGvQKUU_"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \"\"\"\n",
        "    한 에폭 동안 모델 학습\n",
        "\n",
        "    Args:\n",
        "        model: Transformer 모델\n",
        "        iterator: 학습 데이터 로더\n",
        "        optimizer: 옵티마이저 (Adam)\n",
        "        criterion: 손실 함수 (CrossEntropyLoss)\n",
        "        clip: 그래디언트 클리핑 값\n",
        "\n",
        "    Returns:\n",
        "        평균 손실\n",
        "    \"\"\"\n",
        "    # 학습 모드로 설정 (드롭아웃 활성화)\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0  # 에폭 총 손실\n",
        "\n",
        "    # 모든 배치에 대해 반복\n",
        "    for batch in iterator:\n",
        "        # 1. 데이터를 디바이스로 이동\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        target_ids = batch['target_ids'].to(device)\n",
        "\n",
        "        # 2. 그래디언트 초기화 (이전 배치의 그래디언트 제거)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 3. Forward pass: 모델 예측\n",
        "        # output: (output, decoder_attention_scores)\n",
        "        output, _ = model(input_ids)\n",
        "\n",
        "        # 4. 손실 계산\n",
        "        # Cross Entropy Loss 계산\n",
        "        loss = criterion(output.contiguous().view(-1, output.shape[-1]),  # (batch * seq, vocab_size)\n",
        "                         target_ids.contiguous().view(-1)) # (batch * seq)\n",
        "\n",
        "\n",
        "        # 5. Backward pass: 그래디언트 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 6. 그래디언트 클리핑 (그래디언트 폭발 방지)\n",
        "        # 그래디언트의 노름이 clip을 넘지 않도록 조정\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # 7. 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 배치 손실 누적\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # 평균 손실 반환\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    모델 평가 (검증/테스트)\n",
        "\n",
        "    학습과 유사하지만:\n",
        "    - 그래디언트 계산 안 함 (torch.no_grad)\n",
        "    - 파라미터 업데이트 안 함\n",
        "    - 드롭아웃 비활성화 (model.eval)\n",
        "\n",
        "    Args:\n",
        "        model: 평가할 모델\n",
        "        iterator: 검증/테스트 데이터 로더\n",
        "        criterion: 손실 함수\n",
        "\n",
        "    Returns:\n",
        "        평균 손실\n",
        "    \"\"\"\n",
        "    # 평가 모드로 설정 (드롭아웃 비활성화)\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # 그래디언트 계산 비활성화 (메모리 절약, 속도 향상)\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            # 데이터 로드\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            target_ids = batch['target_ids'].to(device)\n",
        "\n",
        "            # Forward pass (그래디언트 계산 안 함)\n",
        "            output, _ = model(input_ids)\n",
        "\n",
        "            # 손실 계산 (학습과 동일)\n",
        "            loss = criterion(\n",
        "                output.contiguous().view(-1, output.shape[-1]),\n",
        "                target_ids.contiguous().view(-1)\n",
        "            )\n",
        "\n",
        "            # 손실 누적\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    # 평균 손실 반환\n",
        "    return epoch_loss / len(iterator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "LWfu3n8CKUU_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWfu3n8CKUU_",
        "outputId": "410e4563-32f7-4b8f-8016-6500b8f63555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Start Model Training\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   1%|          | 1/100 [00:08<14:09,  8.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 1] Model Saved! Valid Loss: 3.489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   2%|▏         | 2/100 [00:16<13:27,  8.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 2] Model Saved! Valid Loss: 3.212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   3%|▎         | 3/100 [00:24<13:02,  8.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 3] Model Saved! Valid Loss: 3.067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   4%|▍         | 4/100 [00:32<12:51,  8.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 4] Model Saved! Valid Loss: 2.973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   5%|▌         | 5/100 [00:39<12:22,  7.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 5] Model Saved! Valid Loss: 2.903\n",
            "\n",
            "Epoch: 05\n",
            "\tTrain Loss: 2.884\n",
            "\tValidation Loss: 2.903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   6%|▌         | 6/100 [00:47<12:19,  7.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 6] Model Saved! Valid Loss: 2.835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   7%|▋         | 7/100 [00:56<12:22,  7.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 7] Model Saved! Valid Loss: 2.791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   8%|▊         | 8/100 [01:03<12:10,  7.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 8] Model Saved! Valid Loss: 2.757\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:   9%|▉         | 9/100 [01:12<12:17,  8.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 9] Model Saved! Valid Loss: 2.720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  10%|█         | 10/100 [01:21<12:38,  8.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 10] Model Saved! Valid Loss: 2.694\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 2.494\n",
            "\tValidation Loss: 2.694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  11%|█         | 11/100 [01:33<14:05,  9.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 11] Model Saved! Valid Loss: 2.678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  12%|█▏        | 12/100 [01:44<14:41, 10.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 12] Model Saved! Valid Loss: 2.660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  13%|█▎        | 13/100 [01:59<16:30, 11.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 13] Model Saved! Valid Loss: 2.640\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  14%|█▍        | 14/100 [02:13<17:39, 12.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 14] Model Saved! Valid Loss: 2.634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  15%|█▌        | 15/100 [02:26<17:46, 12.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 15] Model Saved! Valid Loss: 2.619\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 2.230\n",
            "\tValidation Loss: 2.619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  16%|█▌        | 16/100 [02:41<18:20, 13.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 16] Model Saved! Valid Loss: 2.618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  17%|█▋        | 17/100 [02:59<20:19, 14.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 17] Model Saved! Valid Loss: 2.608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  18%|█▊        | 18/100 [03:13<19:47, 14.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 18] Model Saved! Valid Loss: 2.604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  19%|█▉        | 19/100 [03:28<19:35, 14.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 19] No Improvement (1/5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  20%|██        | 20/100 [03:42<19:09, 14.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 20] No Improvement (2/5)\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 2.038\n",
            "\tValidation Loss: 2.608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  21%|██        | 21/100 [04:00<20:38, 15.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 21] No Improvement (3/5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  22%|██▏       | 22/100 [04:25<23:46, 18.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 22] Model Saved! Valid Loss: 2.599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  23%|██▎       | 23/100 [04:42<23:11, 18.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 23] No Improvement (1/5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  24%|██▍       | 24/100 [05:00<22:34, 17.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 24] No Improvement (2/5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  25%|██▌       | 25/100 [05:17<22:13, 17.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 25] No Improvement (3/5)\n",
            "\n",
            "Epoch: 25\n",
            "\tTrain Loss: 1.886\n",
            "\tValidation Loss: 2.605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  26%|██▌       | 26/100 [05:35<22:05, 17.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 26] No Improvement (4/5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training in Progress:  26%|██▌       | 26/100 [05:53<16:46, 13.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 27] No Improvement (5/5)\n",
            "\n",
            "Early Stopping! (최고 Valid Loss: 2.599)\n",
            "\n",
            "==================================================\n",
            "학습 완료!\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===== 모델 학습 시작 =====\n",
        "\n",
        "# 학습 하이퍼파라미터\n",
        "N_EPOCHS = 100  # 최대 에폭 수\n",
        "CLIP = 1  # 그래디언트 클리핑\n",
        "best_valid_loss = float('inf')  # 최고 검증 손실 (초기값: 무한대)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Start Model Training\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Early Stopping 설정\n",
        "patience = 5  # 성능 개선 없으면 5 에폭 후 종료\n",
        "patience_counter = 0  # 카운터 초기화\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in tqdm(range(N_EPOCHS), desc=\"Training in Progress\"):\n",
        "    # 1. 학습\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "\n",
        "    # 2. 검증\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    # 3. Early Stopping 체크\n",
        "    if valid_loss < best_valid_loss:\n",
        "        # 검증 손실이 개선됨\n",
        "        best_valid_loss = valid_loss  # 최고 기록 갱신\n",
        "        patience_counter = 0  # 카운터 리셋\n",
        "\n",
        "        # 최고 성능 모델 저장\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(f'\\n[Epoch {epoch+1}] Model Saved! Valid Loss: {valid_loss:.3f}')\n",
        "    else:\n",
        "        # 검증 손실이 개선되지 않음\n",
        "        patience_counter += 1  # 카운터 증가\n",
        "        print(f'\\n[Epoch {epoch+1}] No Improvement ({patience_counter}/{patience})')\n",
        "\n",
        "        # Patience 초과 시 조기 종료\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly Stopping! (최고 Valid Loss: {best_valid_loss:.3f})\")\n",
        "            break\n",
        "\n",
        "    # 4. 학습 상태 출력\n",
        "    if (epoch + 1) % 5 == 0:  # 5 에폭마다 출력\n",
        "        print(f'\\nEpoch: {epoch+1:02}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "        print(f'\\tValidation Loss: {valid_loss:.3f}')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"학습 완료!\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "gUxdiw62KUU_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUxdiw62KUU_",
        "outputId": "fbd1f0b9-92bc-4731-d2e2-5f99e1e78ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "테스트 손실: 2.599\n",
            "테스트 Perplexity: 13.449\n"
          ]
        }
      ],
      "source": [
        "# ===== 테스트 세트 최종 평가 =====\n",
        "\n",
        "# 저장된 최고 모델 로드\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "# 테스트 세트 평가\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f\"\\n테스트 손실: {test_loss:.3f}\")\n",
        "print(f\"테스트 Perplexity: {np.exp(test_loss):.3f}\")  # Perplexity: exp(loss)\n",
        "# Perplexity: 모델의 불확실성 지표 (낮을수록 좋음)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8d4299d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "kzPFdbv7KUU_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzPFdbv7KUU_",
        "outputId": "7b918b19-f8ab-4d31-e846-ce952f3e8bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "모델 추론 예제\n",
            "==================================================\n",
            "\n",
            "질문: 짝사랑이랑 연애하고 싶다\n",
            "답변: 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "질문: 오늘 날씨가 안좋아!\n",
            "답변: 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "질문: 뭐 하고 있나요 하고 메시지 보내고 싶어\n",
            "답변: 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "질문: 고기 먹고 싶어\n",
            "답변: 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "질문: 내가 좋아하는 거 알았는데도 나를 대하는게 변함이 없어.\n",
            "답변: 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "질문: 내가 좋아하는 걸 티냈는데 그 사람은 반응이 없어.\n",
            "답변: 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "질문: 요즘 너무 외로워서 힘들어.\n",
            "답변: 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n"
          ]
        }
      ],
      "source": [
        "# ===== 답변 생성 함수 (추론) =====\n",
        "\n",
        "def generate_response(model, question, vocab, max_length=40):\n",
        "    \"\"\"\n",
        "    주어진 질문에 대해 모델이 답변을 생성\n",
        "\n",
        "    생성 방식: Greedy Decoding\n",
        "    - 각 스텝에서 가장 확률이 높은 토큰 선택\n",
        "    - 빠르지만 최적이 아닐 수 있음\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 Transformer 모델\n",
        "        question: 입력 질문 (문자열)\n",
        "        vocab: 어휘 사전\n",
        "        max_length: 최대 생성 길이\n",
        "\n",
        "    Returns:\n",
        "        생성된 답변 (문자열)\n",
        "    \"\"\"\n",
        "    # 평가 모드\n",
        "    model.eval()\n",
        "\n",
        "    # 질문 전처리 및 토큰화\n",
        "    question = preprocess_sentence(question)  # 텍스트 정제\n",
        "    # GPT 스타일: [BOS] + 질문 + [SEP]부터 시작\n",
        "    prefix_ids = ([vocab.BOS_ID] + vocab.encode(question) + \n",
        "                  [vocab.SEP_ID])\n",
        "    trg_ids = prefix_ids  # 질문+SEP부터 시작\n",
        "    # 그래디언트 계산 비활성화\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # 2. 디코더: 답변 생성 (자동 회귀)\n",
        "        # 초기 입력: [BOS] 토큰\n",
        "        trg_ids = [vocab.BOS_ID]\n",
        "\n",
        "        # 최대 길이까지 또는 [EOS] 생성까지 반복\n",
        "        for _ in range(max_length):\n",
        "            # 현재까지 생성된 타겟 시퀀스\n",
        "            trg = torch.tensor(trg_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, cur_len)\n",
        "\n",
        "            # 인과 마스크 생성 (미래 토큰 마스킹)\n",
        "            tgt_len = trg.size(1)\n",
        "            dec_mask = torch.triu(\n",
        "                torch.ones(tgt_len, tgt_len, dtype=torch.bool, device=device),\n",
        "                diagonal=1\n",
        "            )\n",
        "\n",
        "            # 디코더 forward\n",
        "            decoder_output, _ = model.decoder(\n",
        "                trg,\n",
        "                decoder_causal_mask=dec_mask\n",
        "            )\n",
        "\n",
        "            # 최종 출력 레이어\n",
        "            # (1, cur_len, emb_dim) → (1, cur_len, vocab_size)\n",
        "            output = model.prediction_head(decoder_output)\n",
        "\n",
        "            # 마지막 토큰의 예측만 사용\n",
        "            # (1, vocab_size)\n",
        "            next_token_logits = output[:, -1, :]\n",
        "\n",
        "            # 가장 높은 확률의 토큰 선택 (Greedy)\n",
        "            next_token = next_token_logits.argmax(dim=-1).item()\n",
        "\n",
        "            # 생성된 토큰을 시퀀스에 추가\n",
        "            trg_ids.append(next_token)\n",
        "\n",
        "            # [EOS] 토큰이 생성되면 종료\n",
        "            if next_token == vocab.EOS_ID:\n",
        "                break\n",
        "\n",
        "    # 3. 토큰 ID를 문자열로 디코딩\n",
        "    # BOS, EOS \n",
        "    try:\n",
        "        sep_idx = trg_ids.index(vocab.SEP_ID)\n",
        "        answer_ids = trg_ids[sep_idx+1:-1]  # SEP 이후, EOS 제외\n",
        "    except ValueError:\n",
        "        answer_ids = trg_ids[len(prefix_ids):-1]\n",
        "\n",
        "    response = vocab.decode(trg_ids[1:-1])  # [BOS]와 [EOS] 제외\n",
        "    return response\n",
        "\n",
        "\n",
        "# ===== 예제 추론 =====\n",
        "\n",
        "# 테스트 질문들\n",
        "test_questions = [\n",
        "    \"짝사랑이랑 연애하고 싶다\",\n",
        "    \"오늘 날씨가 안좋아!\",\n",
        "    \"뭐 하고 있나요 하고 메시지 보내고 싶어\",\n",
        "    \"고기 먹고 싶어\",\n",
        "    \"내가 좋아하는 거 알았는데도 나를 대하는게 변함이 없어.\",\n",
        "    \"내가 좋아하는 걸 티냈는데 그 사람은 반응이 없어.\",\n",
        "    \"요즘 너무 외로워서 힘들어.\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"모델 추론 예제\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 각 질문에 대해 답변 생성\n",
        "for question in test_questions:\n",
        "    response = generate_response(model, question, vocab)\n",
        "    print(f\"\\n질문: {question}\")\n",
        "    print(f\"답변: {response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "pJ24e9ozmwiE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ24e9ozmwiE",
        "outputId": "3772d80e-c8a8-4dc5-a6c4-0dfa351fc4c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q (from file): 거지됐어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 거짓말 했어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 거짓말을 나도 모르게 자꾸 해\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 거짓말을 하게 돼\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 거짓말이 거짓말을 낳아\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 걱정 없이 살고파\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 걱정 좀 없이 살고 싶다.\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건강 관리\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건강 빨리 회복해야지\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건강검진 왔어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건강검진하러 옴\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건강이 최고\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건강이 최고인 것 같아\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건강하게 다이어트 하는 방법\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건강한 다이어트법\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건너건너 아는 사람인데 연락해도 될까?\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건물주 되고싶어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건물주가 짱인데\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건방져\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건조기 살까봐\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 건조하네\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 걸레질도 해야 돼\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 걸어 가고 있는데 깜깜해서 무서워\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 겁난다\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게으른 동료가 있어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게임 같이 하자고 할까?\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게임 때문에 시간 다갔어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게임 때문에 폰이 점점 느려지는듯\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게임 재미있어.\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게임 지겨워\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게임도 이제 재미없어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게임하고 싶어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 게임하다 시간 다갔어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 겨울 지나 봄이야\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 겨울에는 온천이지!\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 겨울이 가고 봄이 올거야\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 격려 좀 해줘\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 격려가 필요해.\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 견과류 챙겨 먹어야지.\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결국 이런 운명이라니 슬프다\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결정 못하겠어.\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결정은 빠르면 빠를 수록 좋겠지?\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결정은 빠를수록 좋겠지?\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결정을 못 내리겠어. 어떻해\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결정적인 물증이 없어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결혼 했는데.\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결혼 했어\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결혼도 다 돈이다.\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결혼식 가기 귀찮아\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "Q (from file): 결혼식 또 가야돼\n",
            "A (model reply): 짝남이랑 카톡 읽씹한거 같은데 어떡하지?[SEP] 먼저 고백하는게 좋을 것 같아요.\n",
            "\n",
            "==================================================\n",
            "CSV 파일 테스트 완료.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#추가 실험\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "questions_from_file = df['Q'].tolist()\n",
        "questions_to_test = questions_from_file[100:150]\n",
        "for question in questions_to_test:\n",
        "    response = generate_response(model, question, vocab)\n",
        "    print(f\"\\nQ (from file): {question}\")\n",
        "    print(f\"A (model reply): {response}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"CSV 파일 테스트 완료.\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rdOqIzedeQqM",
      "metadata": {
        "id": "rdOqIzedeQqM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "experiment",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.24"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
